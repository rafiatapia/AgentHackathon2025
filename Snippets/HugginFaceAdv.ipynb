{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d4d480f-92f3-459d-80ab-23f382fd1373",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-17 23:09:47.354827: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-10-17 23:09:47.371561: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-10-17 23:09:47.377169: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-10-17 23:09:47.390568: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding,\n",
    "    AutoModel, AutoConfig\n",
    ")\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e7df7d5-3ce9-4a2c-bcc2-bba6f8bfcec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f80fc013430>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Using device: {device}\")\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3b25d59-7d97-4dae-9733-36663f174bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TOKENIZATION EXAMPLE ===\n",
      "Original text: Hello, how are you today?\n",
      "Tokens: ['hello', ',', 'how', 'are', 'you', 'today', '?']\n",
      "Token IDs: [101, 7592, 1010, 2129, 2024, 2017, 2651, 1029, 102]\n",
      "Decoded: [CLS] hello, how are you today? [SEP]\n",
      "--------------------------------------------------\n",
      "Original text: The quick brown fox jumps over the lazy dog.\n",
      "Tokens: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n",
      "Token IDs: [101, 1996, 4248, 2829, 4419, 14523, 2058, 1996, 13971, 3899, 1012, 102]\n",
      "Decoded: [CLS] the quick brown fox jumps over the lazy dog. [SEP]\n",
      "--------------------------------------------------\n",
      "Original text: Tokenization is the process of breaking text into tokens.\n",
      "Tokens: ['token', '##ization', 'is', 'the', 'process', 'of', 'breaking', 'text', 'into', 'token', '##s', '.']\n",
      "Token IDs: [101, 19204, 3989, 2003, 1996, 2832, 1997, 4911, 3793, 2046, 19204, 2015, 1012, 102]\n",
      "Decoded: [CLS] tokenization is the process of breaking text into tokens. [SEP]\n",
      "--------------------------------------------------\n",
      "\n",
      "Batch tokenization:\n",
      "Input IDs shape: torch.Size([3, 14])\n",
      "Attention mask shape: torch.Size([3, 14])\n"
     ]
    }
   ],
   "source": [
    "def tokenization_example():\n",
    "    \"\"\"Demonstrate tokenization process\"\"\"\n",
    "    print(\"\\n=== TOKENIZATION EXAMPLE ===\")\n",
    "        \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "        \n",
    "    sample_texts = [\n",
    "        \"Hello, how are you today?\",\n",
    "        \"The quick brown fox jumps over the lazy dog.\",\n",
    "        \"Tokenization is the process of breaking text into tokens.\"\n",
    "    ]\n",
    "        \n",
    "    for text in sample_texts:\n",
    "        # Tokenize\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        token_ids = tokenizer.encode(text)\n",
    "            \n",
    "        print(f\"Original text: {text}\")\n",
    "        print(f\"Tokens: {tokens}\")\n",
    "        print(f\"Token IDs: {token_ids}\")\n",
    "        print(f\"Decoded: {tokenizer.decode(token_ids)}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "    # Batch tokenization\n",
    "    print(\"\\nBatch tokenization:\")\n",
    "    batch_encoding = tokenizer(sample_texts,padding=True,truncation=True,return_tensors=\"pt\",max_length=128)\n",
    "        \n",
    "    print(f\"Input IDs shape: {batch_encoding['input_ids'].shape}\")\n",
    "    print(f\"Attention mask shape: {batch_encoding['attention_mask'].shape}\")\n",
    "\n",
    "tokenization_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beb38eb1-e0f4-438e-8d11-b37ba5adc74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MODEL COMPARISON EXAMPLE ===\n",
      "\n",
      "Testing model: distilbert-base-uncased-finetuned-sst-2-english\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  I absolutely love this product... -> POSITIVE (1.000)\n",
      "  This is the worst thing I've e... -> NEGATIVE (1.000)\n",
      "  It's okay, nothing special.... -> NEGATIVE (0.819)\n",
      "  Amazing quality and fast deliv... -> POSITIVE (1.000)\n",
      "  Terrible customer service expe... -> NEGATIVE (1.000)\n",
      "\n",
      "Testing model: cardiffnlp/twitter-roberta-base-sentiment-latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  I absolutely love this product... -> positive (0.984)\n",
      "  This is the worst thing I've e... -> negative (0.953)\n",
      "  It's okay, nothing special.... -> neutral (0.599)\n",
      "  Amazing quality and fast deliv... -> positive (0.971)\n",
      "  Terrible customer service expe... -> negative (0.933)\n",
      "\n",
      "Testing model: nlptown/bert-base-multilingual-uncased-sentiment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  I absolutely love this product... -> 5 stars (0.963)\n",
      "  This is the worst thing I've e... -> 1 star (0.962)\n",
      "  It's okay, nothing special.... -> 3 stars (0.847)\n",
      "  Amazing quality and fast deliv... -> 5 stars (0.920)\n",
      "  Terrible customer service expe... -> 1 star (0.759)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'distilbert-base-uncased-finetuned-sst-2-english': [{'text': 'I absolutely love this product!',\n",
       "   'label': 'POSITIVE',\n",
       "   'score': 0.9998854398727417},\n",
       "  {'text': \"This is the worst thing I've ever bought.\",\n",
       "   'label': 'NEGATIVE',\n",
       "   'score': 0.9997859597206116},\n",
       "  {'text': \"It's okay, nothing special.\",\n",
       "   'label': 'NEGATIVE',\n",
       "   'score': 0.818959653377533},\n",
       "  {'text': 'Amazing quality and fast delivery!',\n",
       "   'label': 'POSITIVE',\n",
       "   'score': 0.9998842477798462},\n",
       "  {'text': 'Terrible customer service experience.',\n",
       "   'label': 'NEGATIVE',\n",
       "   'score': 0.9997796416282654}],\n",
       " 'cardiffnlp/twitter-roberta-base-sentiment-latest': [{'text': 'I absolutely love this product!',\n",
       "   'label': 'positive',\n",
       "   'score': 0.9843719601631165},\n",
       "  {'text': \"This is the worst thing I've ever bought.\",\n",
       "   'label': 'negative',\n",
       "   'score': 0.9526245594024658},\n",
       "  {'text': \"It's okay, nothing special.\",\n",
       "   'label': 'neutral',\n",
       "   'score': 0.598635733127594},\n",
       "  {'text': 'Amazing quality and fast delivery!',\n",
       "   'label': 'positive',\n",
       "   'score': 0.9710035920143127},\n",
       "  {'text': 'Terrible customer service experience.',\n",
       "   'label': 'negative',\n",
       "   'score': 0.9334829449653625}],\n",
       " 'nlptown/bert-base-multilingual-uncased-sentiment': [{'text': 'I absolutely love this product!',\n",
       "   'label': '5 stars',\n",
       "   'score': 0.9634515643119812},\n",
       "  {'text': \"This is the worst thing I've ever bought.\",\n",
       "   'label': '1 star',\n",
       "   'score': 0.9616283178329468},\n",
       "  {'text': \"It's okay, nothing special.\",\n",
       "   'label': '3 stars',\n",
       "   'score': 0.8465539216995239},\n",
       "  {'text': 'Amazing quality and fast delivery!',\n",
       "   'label': '5 stars',\n",
       "   'score': 0.9204044938087463},\n",
       "  {'text': 'Terrible customer service experience.',\n",
       "   'label': '1 star',\n",
       "   'score': 0.75850909948349}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def model_comparison_example():\n",
    "    \"\"\"Compare different models on the same task\"\"\"\n",
    "    print(\"\\n=== MODEL COMPARISON EXAMPLE ===\")\n",
    "        \n",
    "    models_to_compare = [\n",
    "            \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "            \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "            \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "    ]\n",
    "        \n",
    "    test_texts = [\n",
    "        \"I absolutely love this product!\",\n",
    "        \"This is the worst thing I've ever bought.\",\n",
    "        \"It's okay, nothing special.\",\n",
    "        \"Amazing quality and fast delivery!\",\n",
    "        \"Terrible customer service experience.\"\n",
    "    ]\n",
    "        \n",
    "    results = {}\n",
    "        \n",
    "    for model_name in models_to_compare:\n",
    "        print(f\"\\nTesting model: {model_name}\")\n",
    "        try:\n",
    "            from transformers import pipeline\n",
    "            classifier = pipeline(\"sentiment-analysis\", model=model_name)\n",
    "                \n",
    "            model_results = []\n",
    "            for text in test_texts:\n",
    "                result = classifier(text)\n",
    "                model_results.append({\n",
    "                    'text': text,\n",
    "                    'label': result[0]['label'],\n",
    "                    'score': result[0]['score']\n",
    "                })\n",
    "                \n",
    "            results[model_name] = model_results\n",
    "                \n",
    "            # Show results for this model\n",
    "            for result in model_results:\n",
    "                print(f\"  {result['text'][:30]}... -> {result['label']} ({result['score']:.3f})\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"  Error with {model_name}: {str(e)}\")\n",
    "        \n",
    "    return results\n",
    "model_comparison_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9990ec7f-44b2-4388-9561-793bd4e25ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "hf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
