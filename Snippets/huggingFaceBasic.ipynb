{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7688475-4e25-4260-bffb-ef17044718aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    AutoModelForQuestionAnswering, AutoModelForTokenClassification,\n",
    "    pipeline, AutoModel\n",
    ")\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5e8d7db-7a4e-49a9-a292-5e39852d8682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdff3b2f410>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Using device: {device}\")\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7288302b-d093-4402-805e-3cb3404e1132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SENTIMENT ANALYSIS DEMO ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample texts and their sentiments:\n",
      "Text: I love this new product! It's amazing!\n",
      "Sentiment: positive (confidence: 0.990)\n",
      "--------------------------------------------------\n",
      "Text: This is terrible, worst experience ever.\n",
      "Sentiment: negative (confidence: 0.945)\n",
      "--------------------------------------------------\n",
      "Text: The weather is okay today, nothing special.\n",
      "Sentiment: positive (confidence: 0.781)\n",
      "--------------------------------------------------\n",
      "Text: Absolutely fantastic work on this project!\n",
      "Sentiment: positive (confidence: 0.980)\n",
      "--------------------------------------------------\n",
      "Text: I'm not sure how I feel about this change.\n",
      "Sentiment: negative (confidence: 0.688)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Analysis\n",
    "print(\"\\n=== SENTIMENT ANALYSIS DEMO ===\")\n",
    "        \n",
    "# Using pipeline (easiest way)\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "        \n",
    "sample_texts = [\n",
    "    \"I love this new product! It's amazing!\",\n",
    "    \"This is terrible, worst experience ever.\",\n",
    "    \"The weather is okay today, nothing special.\",\n",
    "    \"Absolutely fantastic work on this project!\",\n",
    "    \"I'm not sure how I feel about this change.\"\n",
    "]\n",
    "        \n",
    "print(\"Sample texts and their sentiments:\")\n",
    "for text in sample_texts:\n",
    "    result = classifier(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {result[0]['label']} (confidence: {result[0]['score']:.3f})\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4909cb7f-354b-4b1d-8d7a-2a5b8b8ab0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== QUESTION ANSWERING DEMO ===\n",
      "Question: When was Hugging Face founded?\n",
      "Answer: 2016\n",
      "Confidence: 0.982\n",
      "--------------------------------------------------\n",
      "Question: Who are the founders of Hugging Face?\n",
      "Answer: Clément Delangue, Julien Chaumond, \n",
      "and Thomas Wolf\n",
      "Confidence: 0.950\n",
      "--------------------------------------------------\n",
      "Question: What is Hugging Face known for?\n",
      "Answer: transformers library\n",
      "Confidence: 0.685\n",
      "--------------------------------------------------\n",
      "Question: Where is Hugging Face headquartered?\n",
      "Answer: New York City\n",
      "Confidence: 0.996\n",
      "--------------------------------------------------\n",
      "Question: Why Pakistan cricket team sucks?\n",
      "Answer: The company is headquartered \n",
      "in New York City and has raised significant funding from investors\n",
      "Confidence: 0.020\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Question and Answer\n",
    "print(\"\\n=== QUESTION ANSWERING DEMO ===\")\n",
    "        \n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "        \n",
    "context = \"\"\"\n",
    "Hugging Face is a company that develops tools for building applications using \n",
    "machine learning. The company was founded in 2016 by Clément Delangue, Julien Chaumond, \n",
    "and Thomas Wolf. Hugging Face is known for its transformers library, which provides \n",
    "pre-trained models for natural language processing tasks. The company is headquartered \n",
    "in New York City and has raised significant funding from investors.\n",
    "\"\"\"\n",
    "        \n",
    "questions = [\n",
    "\"When was Hugging Face founded?\",\n",
    "\"Who are the founders of Hugging Face?\",\n",
    "\"What is Hugging Face known for?\",\n",
    "\"Where is Hugging Face headquartered?\",\n",
    "\"Why Pakistan cricket team sucks?\"\n",
    "]\n",
    "        \n",
    "for question in questions:\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"Confidence: {result['score']:.3f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecee4371-b187-4c66-92bb-58214bae5ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NAMED ENTITY RECOGNITION DEMO ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Apple Inc. was founded by Steve Jobs in Cupertino, California.\n",
      "Entities found:\n",
      "  - Apple Inc: ORG (confidence: 1.000)\n",
      "  - Steve Jobs: PER (confidence: 0.989)\n",
      "  - Cupertino: LOC (confidence: 0.971)\n",
      "  - California: LOC (confidence: 0.999)\n",
      "--------------------------------------------------\n",
      "Text: The meeting with Microsoft will be held in Seattle next Tuesday.\n",
      "Entities found:\n",
      "  - Microsoft: ORG (confidence: 1.000)\n",
      "  - Seattle: LOC (confidence: 0.999)\n",
      "--------------------------------------------------\n",
      "Text: Barack Obama was the 44th President of the United States.\n",
      "Entities found:\n",
      "  - Barack Obama: PER (confidence: 0.999)\n",
      "  - United States: LOC (confidence: 0.995)\n",
      "--------------------------------------------------\n",
      "Text: Google's headquarters are located in Mountain View, California.\n",
      "Entities found:\n",
      "  - Google: ORG (confidence: 0.999)\n",
      "  - Mountain View: LOC (confidence: 0.995)\n",
      "  - California: LOC (confidence: 0.998)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Entity Recognition\n",
    "print(\"\\n=== NAMED ENTITY RECOGNITION DEMO ===\")\n",
    "        \n",
    "ner_pipeline = pipeline(\"ner\", model=\"dbmdz/bert-large-cased-finetuned-conll03-english\",aggregation_strategy=\"simple\")\n",
    "        \n",
    "sample_texts = [\n",
    "            \"Apple Inc. was founded by Steve Jobs in Cupertino, California.\",\n",
    "            \"The meeting with Microsoft will be held in Seattle next Tuesday.\",\n",
    "            \"Barack Obama was the 44th President of the United States.\",\n",
    "            \"Google's headquarters are located in Mountain View, California.\"\n",
    "        ]\n",
    "        \n",
    "for text in sample_texts:\n",
    "    entities = ner_pipeline(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(\"Entities found:\")\n",
    "    for entity in entities:\n",
    "        print(f\"  - {entity['word']}: {entity['entity_group']} (confidence: {entity['score']:.3f})\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22a7cd12-2395-4178-88d7-fb78f628899a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEXT SUMMARIZATION DEMO ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary:\n",
      "Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of \"intelligent agents\" As machines become increasingly capable, tasks considered to require \"intelligence\" are often removed from the definition of AI.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== TEXT SUMMARIZATION DEMO ===\")\n",
    "        \n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "        \n",
    "long_text = \"\"\"\n",
    "        Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to \n",
    "        the natural intelligence displayed by humans and animals. Leading AI textbooks define \n",
    "        the field as the study of \"intelligent agents\": any device that perceives its environment \n",
    "        and takes actions that maximize its chance of successfully achieving its goals. \n",
    "        Colloquially, the term \"artificial intelligence\" is often used to describe machines \n",
    "        that mimic \"cognitive\" functions that humans associate with the human mind, such as \n",
    "        \"learning\" and \"problem solving\". As machines become increasingly capable, tasks \n",
    "        considered to require \"intelligence\" are often removed from the definition of AI, \n",
    "        a phenomenon known as the AI effect. A quip in Tesler's Theorem says \"AI is whatever \n",
    "        hasn't been done yet.\" For instance, optical character recognition is frequently \n",
    "        excluded from things considered to be AI, having become a routine technology. \n",
    "        Modern machine learning techniques are heavy on data and require substantial \n",
    "        computational power.\n",
    "        \"\"\"\n",
    "        \n",
    "summary = summarizer(long_text, max_length=100, min_length=30, do_sample=False)\n",
    "print(\"\\nSummary:\")\n",
    "print(summary[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "130b514d-e943-4d7c-8b5f-620e7a040e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEXT GENERATION DEMO ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: The future of artificial intelligence is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1: The future of artificial intelligence is going to be of big, beautiful, and exciting proportions.\"\n",
      "\n",
      "Explore further: Robots will find smarter life\n",
      "Generation 2: The future of artificial intelligence is very bright and tomorrow could usher in another epoch. In an interview with the New York Observer, Joseph Kroeber, the director of the Carnegie Institution for Science, described a future with \"a world where people would spend more money buying the technologies that will make us happy and which will lead to greater prosperity.\"[42]\n",
      "\n",
      "Kroeber warned of the impact of technological change on human society. The Industrial Revolution unleashed an unprecedented concentration of capital and resources needed for a society that began with the expansion of machinery and was still expanding at a rapid rate.\n",
      "\n",
      "He pointed out that the rise of mass media and the increase in the Internet make technology the main driver of changing social life, and could lead us to create a more equitable, connected world, because technology will advance the way society is currently governed by the state.\n",
      "\n",
      "Kroeber's view is consistent with the more than 20 countries that have embarked on their own digital revolution, most notably the United States and Hong Kong. The United States has adopted digital health technology to promote its own growth. In China, the National University of Science and Technology has built a computer system to monitor and help educate researchers about a medical condition, providing a way for people to communicate as they are learning new skills. In the Netherlands, in a similar project, the National University of Electrical and Computer Engineering began building an internet-based data centre, and then launched a digital health product, the Mind-Body Clinic. In Germany, a research team led by the University of Technology Berlin developed a new kind of internet system that is increasingly capable of tracking and studying subjects while providing education support.[43]\n",
      "\n",
      "For example, in 2014 a German group called the Düsseldorf University Digital Health Association was asked to develop a patient population questionnaire. The questionnaire asked a self-selected number of questions about their general health. The objective was to assess how many people would like to meet, at different stages of their lives, using the smartphone of a doctor or nurse practitioner and using a computer to share that information with patients. It also sought to assess how effective such information would be for improving the health of people.\n",
      "\n",
      "In 2015 at the Copenhagen conference of the Düsseldorf University Digital Health Association, the project began in earnest. Participants developed a system that could be used by patients and physicians to track their progress and evaluate their overall health with information about what was likely to be the worst outcome. The system was originally developed for the European Commission and the European Commission has been developing its own digital privacy regulation for the Internet since the 2011 Internet and Privacy Act was enacted. This regulation mandates the establishment of independent privacy protection organizations that will take responsibility for monitoring and monitoring the use of personal information collected by the government and the private sector on a daily basis, whether by telecommunications companies or through third parties. The project received a $22 million award from the German government and another $15 million was received by the Belgian government.\n",
      "\n",
      "The initiative will be completed in 2016. Its objective remains to develop and conduct research that will demonstrate the effectiveness of digital health care in the short and long run.[44]\n",
      "\n",
      "For example, in 2006 the European Commission raised the awareness of a new concept to measure people's physical and mental health with information about their activities. That would allow researchers to compare information about activities of individuals who have an increased risk of developing Alzheimer's disease with personal information about those who have a similar condition.[45]\n",
      "\n",
      "In 2015 the European Parliament's Working Committee on Health in the EU (the DOUGA ) began a European Monitoring Group on the Health Sector in the ECS to investigate digital health conditions with respect to information gathered through monitoring. The report also identified and reported on potential measures needed to prevent and treat the most common digital diseases. In November 2015 the European Commission adopted a new directive that also raised awareness of the use of digital health records in the health sector.\n",
      "\n",
      "The EU's first digital health regulation, the Directive on the Information of Member States, was adopted by a majority of the 473 of 1232 Member States that signed the text. It was signed by the seven member States of the European Parliament and includes key provisions for the regulation, according to data privacy and Internet.\n",
      "\n",
      "The European Parliament's Committee on Information Technology and the Information Technology of the Commission, together with the DOUGA, held an online meeting in November 2015 in New York City. The European Commission announced that it would vote to approve the European Data Protection Directive, which the DOUGA has been negotiating with Member States in the past. The European Parliament's report, titled Member States' Digital Health Rules for 2012, was jointly drafted by the European Parliament's data privacy division, the Data Protection and Social Welfare Division and the European Commissioner for Information Technologies. The Directive provided for a single, highly efficient and safe online data protection system to be built and operated without the use of data on individual users.\n",
      "--------------------------------------------------\n",
      "Prompt: In the world of machine learning,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1: In the world of machine learning, these results seem to have been particularly significant, at least in academia. In 2012, in an article in the Proceedings of the National Academy of Sciences, Professor Daniel Jadakas of the University of New Mexico conducted a study with 17 different researchers and found that their dataset was much more accurate.\n",
      "\n",
      "Advertisement\n",
      "\n",
      "In 2011, the Stanford Artificial Intelligence Lab's Jadakas et al published a long paper with an additional scientist looking for some interesting results:\n",
      "\n",
      "A significant group of researchers from MIT, Yale and the University of Pittsburgh, with the support of the AI lab, have shown that when an intelligent neural network works successfully with a large set of data, there's an increased rate of activation in the neurons of the left and right, resulting in a higher level of creativity and self-awareness. In other words, this network can build up to more than a hundred million neurons, which are then trained on a very large array of input data, and then train one or more more to generate some particular kind of algorithm, allowing an AI to solve its own problems when in close proximity to the input data and, in such cases, be more accurate than the control, the real world problem. The new work has enormous implications for the way artificial intelligence and deep learning, and how we're going to learn about human behavior on the fly, and how they can help us grow our data sets to better understand these machines and how they can improve upon human reasoning skills.\n",
      "\n",
      "Advertisement\n",
      "\n",
      "Advertisement\n",
      "\n",
      "In an interesting twist, Google and IBM also had similar results, too, in 2012. As the Journal of the National Academy of Sciences notes (emphasis mine): \"As the company's research shows, this new method of machine learning can work in a wide variety of scenarios, including tasks such as natural language processing, for both real- and computationally demanding tasks such as natural language processing as well as the data processing of complex tasks such as mathematical operations as well as the data processing of machine learning models to better analyze the data.\"\n",
      "\n",
      "As the Journal points out:\n",
      "\n",
      "A large, parallel data set contains millions of neurons and about one thousand networks. As a consequence of the massive number of neurons, and the wide number of interactions between the neurons, there will be different ways of learning and processing neural networks at large scales. These are known as supervised learning networks (PSNN) – the process in which the most accurate and efficient technique, which can be applied when used in the context of real-world AI, is presented to a human agent. Some PSNs have shown up in a large number of other models that have not yet proven in any sense a direct representation of real-world human behavior. \"And for now, on the surface, the main results of this new approach are promising, because we show that this is the first truly synthetic machine network that can overcome human error errors using a single algorithm. The research has implications for a wide range of different scientific disciplines, including information science, biological computing, cognitive science, applied computational science, and machine learning.\"\n",
      "\n",
      "Advertisement\n",
      "\n",
      "Not surprisingly, the new system has a high failure rate, and will probably never work as advertised. It seems like a pretty big bet to build a machine in real-world order. Indeed, with no clear path to safety and very little incentive to produce AI applications for human use, it's only natural to conclude that, under these conditions, it'll look a lot like a conventional machine learning algorithm.\n",
      "\n",
      "Advertisement\n",
      "\n",
      "In fact, it's pretty obvious what they're looking for from Google and the AI lab, so there's no need to spend time thinking about how exactly one of our AI programs is supposed to work. Indeed, it's not clear how they'll fit either. \"The key thing is that the models will generally be highly flexible, so if we can't get a handle on how they will fit in our database or how the artificial neural networks that we'll run will fit, we'll have to consider how to handle such scenarios,\" says Daniel Kopp of the University of Utah, who led a study led by the University of Utah machine learning group that was recently led by Prof. Kopp, including the aforementioned MIT papers (emphasis mine).\n",
      "\n",
      "What's clear from that is there is an interesting and possibly unanticipated future for AI in the way the human brain works. AI won't be an abstract knowledge model, rather it's a full-fledged model that aims to understand the mind and its internal environment and what it needs in order to make progress. The current models have many different features, but they all share the same goal: to teach our brains to do something about problems we'd normally learn to solve. It's possible that in a year's time, we're going to see this model grow as widely as any existing method for modeling the mind. It probably will be a lot quicker than\n",
      "Generation 2: In the world of machine learning, there's only so much flexibility to manage and test a set of data. Even this bit of flexibility can be seen in an analogy with Java, wherein there is an array of functions:\n",
      "\n",
      "class MyInt [A] def __init__(self, i, j, g): # initialize class Int[A] def __del__(self): self.j = j self.g.x += 10 self.j.y += 12 self.g.z += 32 def __str__(self): # return self self.j.x = self.g.z def __unix__(self, x): # copy from class Int int i = -1 # 1 byte of data (1 if all integer, 8 otherwise) int j = -1 # 2 byte of data (2 if all integer, 9 then this is byte-only) int w = int(self.j.x, self.j.y, self.g, self.j.z, self.g.x) for i = 0: int(f1, f2, f3) w = w + x + 1 for f1 and f2 in self.i: if f1[t(t)==f1[f2[t(t)]]) and f2[t(f1,f2]==f1[f2[T(t]][f1[F(t))])] and f1[f2[t(f1,f2[F(t)][f1[f2[F(t)][F(t)]]]] == 2 + 1) self.j.x += 1 p = f1[w/x + 1] p += W ( x > v) self.j.y += 8 p += P ( z > v) # set the initial float, set the length self.j.x += g self.j.y += w * 5 print(p )\n",
      "\n",
      "In this example, we have a set of variables, in this case, to initialize the initial int we're using a new string. As well, it's a way to pass values to the next function in the call – what we have created at runtime. Of course, in reality, it's a better approach, if one chooses to use the 'int and int constants'. When a function is called, the function gets replaced with the first one in its scope, then some other functions of its scope, and so on. Note that all the variables in this class are in scope – the type is always the same, the value of each of the first two arguments is always in scope, and so on.\n",
      "\n",
      "I wanted to use this example in order to show how to use the 'int and int constants' approach as we've already seen.\n",
      "\n",
      "In each instance, the int and int constants are stored in the variable 'char'. I then copied each 'int.h' to /var/lib/int.h and passed around the rest of the code to the variable's constructor ( 'int.h' ).\n",
      "\n",
      "I did this with an important fact: because of our first assignment to 'int', both initial and sum are in constant scope. As we've already seen, if we're returning a value for 'this.', the value of the variable 'this' itself is outside 'normal' int arithmetic.\n",
      "\n",
      "There are a few differences between the example above and the one described above –\n",
      "\n",
      "initial.\n",
      "\n",
      "Summing with the return keyword means that 'this' does not have to happen directly in the main loop, but must be executed a second time. This is useful if there are arguments that need to be printed, or if we want to have the function return the first thing we did in the main loop. It's also useful to remember that, in order to wrap something as long as we can think of using a word starting with 'e', we start with 'e'. In our example, at first, 'e' is printed only once, and in the main loop, we're doing a double-quote ('e' always ends with 'e'. This is how we could do it in our function arguments, 'e' must end with (e) that we want to print later.\n",
      "\n",
      "constant.\n",
      "\n",
      "Unlike 'int,' this means 'this' can end with (e) just once. This happens within the main loop, and then will be displayed as a double-quote ('e' usually ends with 'e'). In this case, 'e' is not a variable, but is a constant.\n",
      "\n",
      "When we wrap a variable outside a main loop with 'e'.\n",
      "\n",
      "I've noticed that I've been writing 'add to the end' to get 'add to the end\n",
      "--------------------------------------------------\n",
      "Prompt: Natural language processing helps us\n",
      "Generation 1: Natural language processing helps us think while we're processing these sentences. We'll start with the basic syntax. These basics are all pretty basic (this is also true for the more advanced syntax).\n",
      "\n",
      "\"Language processing\" is where we start trying to understand how sentences are connected between their words and sentences. This has been well documented in several papers such as \"Procedural Analysis Reveals Linkages Between the Language, Verb and Action Type\", and \"Language Processing in a Generalized Approach to Sentences\". We'll be more advanced in terms of using this process by building language processing packages. This package provides language learning tools that are well documented and do more than just help us understand the complex relationship between words and sentences.\n",
      "\n",
      "Language Processing\n",
      "\n",
      "We'll be using language processing in the following cases:\n",
      "\n",
      "One sentence. It might be short and ambiguous here, the context might be fuzzy between verbs, and a certain verb might be related to a situation.\n",
      "\n",
      "A word is short and ambiguous in a verb or phrase or to make a noun easier to understand.\n",
      "\n",
      "If a verb is one of the verbs in the list above:\n",
      "\n",
      "\"The money\", that is our current situation in society.\n",
      "\n",
      "The verb is ambiguous or means something that \"gets the verb on the spot\" or \"gets it out of my head\".\n",
      "\n",
      "For example, let's say the person wanted to get his son vaccinated.\n",
      "\n",
      "To find this person this sentence would look like:\n",
      "\n",
      "\"The family is sick\".\n",
      "\n",
      "Here we're saying something like:\n",
      "\n",
      "\"The money isn't really a problem for the family, but in the future this might become a problem for everyone.\" (\"The family is sick, they can't afford the vaccine\").\n",
      "\n",
      "To get a higher level of familiarity with language processing in our code we use the \"Syntax Analysis Tool\" or, in other words, the Linguistics Classifier.\n",
      "\n",
      "Language Processing Linguistics\n",
      "\n",
      "First I need to describe this package as a set of tools to help us with understanding the fundamental data structure of our corpus text. For a good introduction to the tool I recommend the Knowledge Base Online project and its blog.\n",
      "\n",
      "The Tool\n",
      "\n",
      "The simplest way to understand the language processing problem is to analyze the corpus text of the corpus text. For example, a sentence of a language is a list of words that we can understand as nouns or verbs. These words are written on a lexical block:\n",
      "\n",
      "{ \"name\" : \"John\". \"city\" : New York }\n",
      "\n",
      "The syntax of a corpus text (the list of words), is given by the syntax attribute attribute \"name\".\n",
      "\n",
      "When we read the block we are looking for a simple syntax or something more complex (the nouns and verbs). Here we have an input of one of these simple syntax attributes and we interpret it with several words as noun (like \"name\", \"city\") and verb (like \"name\"). We can also interpret other data as noun, like \"city\"; say, \"I bought this car\". When we understand the value of verbs it can also be the \"language\" (like \"words\"), like a set of keywords (like \"class\", \"phrase\"), the \"sentences\", or other variables.\n",
      "\n",
      "An expression like \"hello\" is a list of two simple verbs and its arguments (which are related to the data in the corpus text) (a vocabulary or a dictionary); we can also have this expression write \"hello\" as a different language. For example, the syntax of \"hey name, I'm in the city\" would be the same as this statement:\n",
      "\n",
      "\"Hello, I am in the city\".\n",
      "\n",
      "Or as in \"Hello, I am in the city\". When we understand the value of a language the syntax is just a list of words that contain a new word. For example, when we interpret a list as \"hello\" it is the same as a group of words:\n",
      "\n",
      "\"Hello, I am in the city\". \"Hello, I am in the city\". \"Hey name, I'm in the city\".\n",
      "\n",
      "For example, the syntax is as like this sentence \"hello\":\n",
      "\n",
      "\"Hello, I am in the city\". \"Hi, we used to live here.\" \"Hi, we use to live here.\" \"Hey name, we used to live here\".\n",
      "\n",
      "A rule in the \"A\" code would be something like this:\n",
      "\n",
      "[{ \"name\" : \"Eli\"; \"city\" : New York },{ \"name\" : \"Michele\"; \"city\" : New Delhi }]\n",
      "\n",
      "We can read the sentence by typing all of these same words at the same time for the first time:\n",
      "\n",
      "[{ \"name\" : \"John\"; \"city\" : New York }]\n",
      "\n",
      "The syntax\n",
      "Generation 2: Natural language processing helps us to recognize our language as natural, and how it fits into the natural world at the moment. To help us recognize the language we use, these principles can be applied together with natural language processing strategies for our daily lives.\n",
      "\n",
      "We can also use natural language processing to learn how our natural language is used. As we learn a language, we learn more about the language, its context, and its actions in real-life situations.\n",
      "\n",
      "It's important to keep in mind this process of natural language processing works for different situations. Sometimes it works for a certain situation. Sometimes it doesn't. So, these processes of natural language processing work for different situations. It's better to know what this process is like first, then take this opportunity to learn your way around the language.\n",
      "\n",
      "A great way I see natural language processing are a number of ways we can help children understand the language better. These include:\n",
      "\n",
      "Learning our natural language to play\n",
      "\n",
      "Learning to use the English language in class\n",
      "\n",
      "Learning to avoid making decisions\n",
      "\n",
      "Learning how to speak with language\n",
      "\n",
      "Learning how to use social media in class\n",
      "\n",
      "Learning to see and communicate with the spoken English language\n",
      "\n",
      "Learning words that are often difficult to understand in the real world\n",
      "\n",
      "Learn what works with a natural language\n",
      "\n",
      "Learn to use our natural language as a learning tool\n",
      "\n",
      "Here are three different natural language processing methods I use to teach myself or others about the natural language.\n",
      "\n",
      "Reading the text from the English dictionary\n",
      "\n",
      "One of my favorite \"sounds like\" natural language processing methods is the \"write it to me\" method. Here are some examples of how it is done:\n",
      "\n",
      "Once you are sure you're reading an English text, we translate a number of important parts of it to English – and it sounds like English sounds like English. Then we move on to use the words to represent the original words.\n",
      "\n",
      "The process works by reading a portion of the text from a natural language dictionary to evaluate the meaning. If it sounds hard, put it onto a table and test it against language labels. If it sounds confusing, put it into your personal writing to teach.\n",
      "\n",
      "If we successfully understand a natural language, our natural language learning can actually be improved greatly. Here are a few examples of what learning the natural language could help us for the following:\n",
      "\n",
      "If you're thinking about learning something new about your language, you can do that from the dictionary, and you'll be learning what the person is saying instead of what they are saying. If you're a linguist, you can see that your natural language learning can actually improve those processes to a degree.\n",
      "\n",
      "Knowing Your Language with Social Media\n",
      "\n",
      "Most of us would call this natural language learning tool. Here are some ways we can learn how to utilize social media to communicate with our natural language and give it some context.\n",
      "\n",
      "This is a very cool part of the entire evolution of natural language processing and is actually something I can do everyday. I will demonstrate how my natural language learning works with social media to make it a lot easier for me to connect with my kids, and hopefully be able to say something to them. I don't require any help from a teacher to use their natural language, but I'll try and make this something that you can use.\n",
      "\n",
      "So, what you can do:\n",
      "\n",
      "To turn on social media, you can do this by following the instructions on this page. If you find that the language supports the social media, just go visit your natural language and get a few things to connect.\n",
      "\n",
      "Also if you don't find anything useful about the social media, you can follow these steps by creating a message on your social media accounts (you can also use the system by following the steps below if you aren't using social media already).\n",
      "\n",
      "Next I'll also show you how to learn English in the real world as well as how to use our natural language and language processing strategies.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def text_generation_demo():\n",
    "    \"\"\"Demonstrate text generation\"\"\"\n",
    "    print(\"\\n=== TEXT GENERATION DEMO ===\")\n",
    "        \n",
    "    generator = pipeline(\"text-generation\", model=\"gpt2\",max_length=100,num_return_sequences=2)\n",
    "    prompts = [\n",
    "            \"The future of artificial intelligence is\",\n",
    "            \"In the world of machine learning,\",\n",
    "            \"Natural language processing helps us\"\n",
    "    ]\n",
    "        \n",
    "    for prompt in prompts:\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        generated_texts = generator(prompt)\n",
    "        for i, text in enumerate(generated_texts):\n",
    "            print(f\"Generation {i+1}: {text['generated_text']}\")\n",
    "        print(\"-\" * 50)\n",
    "text_generation_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ce15b6-8bc4-4a50-b057-fd5cfcda3a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "hf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
